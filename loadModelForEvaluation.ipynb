{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook is not meant to run stand alone, but is %run from *all* the other analysis files in this directory\n",
    "\n",
    "It assumes the notebooks that %run this file define <br>\n",
    "pickledParamFile\n",
    "before running. pickledParamFile is the parameter file stored by RNNControl, and includes all the necessary information to reconstruct and run the saved models. <br>\n",
    "Note that this file defines a generate() function that can synthesize sound under continuously changing conditioning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transform\n",
    "\n",
    "import audioDataloader.dataloader as dataloader\n",
    "from audioDataloader.transforms import mulawnEncode,mulaw,array2tensor,dic2tensor,injectNoise,normalizeDim\n",
    "from paramManager import paramManager\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "\n",
    "import pickle\n",
    "\n",
    "from platform import python_version\n",
    "print(\"python version {}\".format(python_version()))\n",
    "\n",
    "import torch\n",
    "print(\"torch version {}\".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Params  \n",
    "<a id=\"dataparams\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "import pprint\n",
    "\n",
    "assert(pickledParamFile)\n",
    "print(\"will read params from {}\".format(pickledParamFile))\n",
    "\n",
    "modelDir = os.path.dirname(pickledParamFile)\n",
    "print(\"modelDir is {}\".format(modelDir))\n",
    "\n",
    "with open(pickledParamFile, 'rb') as input:\n",
    "    params = pickle.load(input) \n",
    "\n",
    "#print(json.dumps(params, indent=3))\n",
    "print(\"\\n Saved Parameters:\\n\")\n",
    "pprint.pprint(params)\n",
    "\n",
    "#If loadModelFileName is not defined or is None, grab the latest\n",
    "try: loadModelFileName\n",
    "except NameError: loadModelFileName = None\n",
    "    \n",
    "#load the last saved model, or use one specified by program %running this notebook \n",
    "loadModelFile = modelDir + '/' + params['savedModel'] if not loadModelFileName else modelDir + '/' + loadModelFileName\n",
    "print(\"setting loadModelFile to {}\".format(loadModelFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.myUtils import octaves\n",
    "#mapping for the way we'll specify parameters for generating tones\n",
    "midi=octaves(params['lowNote'], params['hiNote']) #default maps midi notenums to freq, norm [0,1] to [p1, p2]\n",
    "\n",
    "k_mincps=midi.param2freq(params['lowNote']) # 523 # 42 samples per period at 22kHz\n",
    "k_maxcps=midi.param2freq(params['hiNote']) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k_rootoutname='./output'\n",
    "\n",
    "seqLen=params['seqLen']\n",
    "sr = 16000\n",
    "\n",
    "#Generation parameters\n",
    "#*************************************\n",
    "max_length = seqLen*48\n",
    "\n",
    "# Cuda\n",
    "#*************************************\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out the available conditional parameters first\n",
    "#*************************************\n",
    "pm = paramManager.paramManager(params['datadir'], params['paramdir'])\n",
    "datafiles = pm.filenames(params['datadir'])\n",
    "paramfile = pm.getParams(datafiles[0]) \n",
    "\n",
    "notpresent=[x for x in params['props'] if x not in paramfile.keys()]\n",
    "assert 0 == len(notpresent), \"props {} are not in the dataparam files\".format(notpresent)\n",
    "\n",
    "#print(paramfile.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset & dataloader\n",
    "#*************************************\n",
    "audiocoding = mulawnEncode(256,0,1) #initialize the mu-law encodings\n",
    "targetcoding = mulaw(256)\n",
    "rescalePitch = normalizeDim('midiPitch',params['lowNote'],params['hiNote'])\n",
    "rescalePressure1 = normalizeDim('pressure1',0,0.9)\n",
    "#rescalePressure2 = normalizeDim('pressure2',.1,1)\n",
    "\n",
    "\n",
    "testdataset = dataloader.AudioDataset(sr,params['seqLen'],params['stride'],\n",
    "                                  csvfile = None if 'csvfile' not in params else params['csvfile'],    \n",
    "                                  datadir  = None if 'csvfile' in params else params['datadir'],\n",
    "                                  extension= None if 'csvfile' in params else 'wav',\n",
    "                                  paramdir=params['paramdir'],prop=params['props'],\n",
    "                                  transform=transform.Compose([array2tensor(torch.FloatTensor)]), \n",
    "                                  param_transform=transform.Compose([ rescalePitch,dic2tensor(torch.FloatTensor)]),\n",
    "                                  target_transform=transform.Compose([array2tensor(torch.LongTensor)]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testdataset,\n",
    "                                          batch_size=1, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4,\n",
    "                                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model\n",
    "#*************************************\n",
    "class RNN(nn.Module):\n",
    "    # input size - the number of \"classes\"\n",
    "    def __init__(self, input_size, cond_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.cond_size = cond_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers #no. of stacked GRU layers\n",
    "\n",
    "        self.i2h = nn.Linear(input_size+cond_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "\n",
    "    # input and cv are each one sequence element \n",
    "    def forward(self, input, hidden, batch_size=1):\n",
    "        #print(\"input size is \" + str((input.size())))\n",
    "        \n",
    "        h1 = self.i2h(input)\n",
    "        #print(\"size of h1 is \" + str(h1.size()))\n",
    "        \n",
    "        h_out, hidden = self.gru(h1.view(batch_size,1,-1), hidden)\n",
    "        #print(\"h_out\"+str(h_out.size()))\n",
    "        \n",
    "        output = self.decoder(h_out.view(batch_size,-1))\n",
    "        #print(\"output2\"+str(output.size()))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    # initialize hiddens for each minibatch\n",
    "    def init_hidden(self,batch_size=1):\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(model,max_length,primer=None,paramvect=None, returnHiddenSequence=False):\n",
    "    \"\"\"Generate a signal using the provided model.\n",
    "    @param max_length of the synthesized signal\n",
    "    @param primer a torch.tensor of shape (batch=1, primer_signal=primer_length, num_inputs=1+num_cond_parameters)\n",
    "    @paramvect conditioning parameters  (a function of i, sample number)\n",
    "    \"\"\" \n",
    "    if returnHiddenSequence :\n",
    "        hs=[]\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for p_inp,target in test_loader:\n",
    "            if primer is not None:\n",
    "                # must clone else primer is changed outside this function\n",
    "                p_inp.data=primer.clone()\n",
    "            seq = np.copy(p_inp[0,:,0])  #extract the original sample\n",
    "            \n",
    "            seq_mu = audiocoding(seq)  #mu-law\n",
    "            p_inp[0,:,0] = array2tensor(torch.FloatTensor)(seq_mu) #now we have both the original and mu-lawed samples\n",
    "            break   \n",
    "        generated = seq # before mu-law audio encoding\n",
    "       \n",
    "        p_inp = p_inp.to(device)\n",
    "        #print(\"p_inp\",p_inp)\n",
    "\n",
    "        hidden = model.init_hidden()\n",
    "        if returnHiddenSequence :\n",
    "            hs.append(torch.squeeze(hidden).cpu().numpy())\n",
    "\n",
    "            \n",
    "        for j in range(seqLen-1):  #build up hidden state\n",
    "            _, hidden = model(p_inp[:,j,:],hidden)\n",
    "        inp = p_inp[:,-1,:]  #feed the last value as the initial value of the actual generation\n",
    "        \n",
    "        #print(\"last inp from primer\",inp)\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            outputs, hidden = model(inp,hidden)\n",
    "            \n",
    "            outputs = nn.functional.log_softmax(outputs,dim=1)\n",
    "            topv, topi = outputs.detach().topk(1)  #choose the strongest activation\n",
    "            #print(topv,topv.shape)\n",
    "            predicted_sample = targetcoding.index2float(topi)\n",
    "            \n",
    "            generated = np.append(generated,predicted_sample)\n",
    "            \n",
    "            inp[:,0] = torch.from_numpy(audiocoding([predicted_sample])).type(torch.cuda.FloatTensor)\n",
    "            #print(\"input\",inp)\n",
    "            #print(\"shape\",inp.shape)\n",
    "            if paramvect is not None:\n",
    "                inp[:,1:] = torch.tensor(paramvect(i))\n",
    "                #print(\"input2\",inp)\n",
    "                \n",
    "            if returnHiddenSequence :\n",
    "                hs.append(torch.squeeze(hidden).cpu().numpy())\n",
    "                                       \n",
    "        if returnHiddenSequence :\n",
    "            return generated, hs\n",
    "        else :\n",
    "            return generated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the network, optimizer and objective func\n",
    "#*************************************\n",
    "rnn = RNN(input_size=1,cond_size=len(params['props']),hidden_size=params['hiddenSize'],output_size=256,n_layers=params['nLayers']).to(device)\n",
    "print(\"**************************************************************************************************.\")\n",
    "print(\"Will load model: {}\".format(loadModelFile))\n",
    "rnn.load_state_dict(torch.load(loadModelFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "snn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
